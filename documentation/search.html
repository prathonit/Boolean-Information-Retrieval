<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>search API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>search</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from index import InvertedIndex, PermutermIndex
from crawl import Dataset


def tokenize(s):
    &#34;&#34;&#34;
    Method to tokenize a string
    :cvar
    &#34;&#34;&#34;
    i = 0
    length = len(s)
    tokens = []
    temp = &#34;&#34;
    while i &lt; length:
        if s[i] == &#39; &#39; or s[i] == &#39;(&#39; or s[i] == &#39;)&#39;:
            if len(temp):
                tokens.append(temp)
            if s[i] == &#39;(&#39; or s[i] == &#39;)&#39;:
                tokens.append(s[i])
            temp = &#34;&#34;
        else:
            temp += s[i]
        i += 1
    if len(temp):
        tokens.append(temp)
    return tokens


class Postings:
    @staticmethod
    def logical_and(l1, l2):
        &#34;&#34;&#34;
        Perform logical and of two lists
        :param l1:
        :param l2:
        :return:
        &#34;&#34;&#34;
        output = []
        i = 0
        j = 0
        len1 = len(l1)
        len2 = len(l2)
        while i &lt; len1 and j &lt; len2:
            if l1[i] == l2[j]:
                output.append(l1[i])
                i += 1
                j += 1
            elif l1[i] &lt; l2[j]:
                i += 1
            else:
                j += 1
        return output

    @staticmethod
    def logical_or(l1, l2):
        &#34;&#34;&#34;
        Performs logical or of two lists
        :param l1:
        :param l2:
        :return:
        &#34;&#34;&#34;
        output = []
        hash_map = {}
        for i in l1:
            hash_map[i] = 1
        for i in l2:
            hash_map[i] = 1
        for key in hash_map:
            output.append(key)
        return output


class Parser:
    &#34;&#34;&#34;
    Parser class parses a search string, retrives the required postings lists and returns the final search result.
    It also performs stop-word removal, stemming and tokenization of the search terms.
    &#34;&#34;&#34;
    def __init__(self):
        self.ps = PorterStemmer()
        self.look_up = LookUp()
        self.reserved_keywords = [&#34;AND&#34;, &#34;OR&#34;, &#34;NOT&#34;, &#34;(&#34;, &#34;)&#34;]

    def pre_process(self, s):
        &#34;&#34;&#34;
        Removes the stop words, stems and tokenize the tokens
        :param s:
        :return:
        &#34;&#34;&#34;
        if len(s) and s[0] != &#39;(&#39;:
            s = &#34;(&#34; + s + &#34;)&#34;
        tokens = tokenize(s)
        for i in range(len(tokens)):
            if tokens[i] not in self.reserved_keywords:
                tokens[i] = self.ps.stem(tokens[i].lower())
        return tokens

    @staticmethod
    def check_format(tokens):
        &#34;&#34;&#34;&#34;
        Checks if the format of the query is correct
        :tokens
        &#34;&#34;&#34;
        tokens_stack = []
        for t in tokens:
            tokens_stack.append(t)
            if tokens_stack[-1] == &#34;)&#34;:
                flag = 0
                while len(tokens_stack) and not flag:
                    if tokens_stack[-1] == &#34;(&#34;:
                        flag = 1
                    tokens_stack.pop()
                if flag == 0:
                    return False
        return True

    def evaluate_query_terms(self, tokens):
        &#34;&#34;&#34;
        Evaluate the query terms, replaces the keywords by the posting lists
        :cvar
        &#34;&#34;&#34;
        evaluated_tokens = []
        i = 0
        while i &lt; len(tokens):
            t = tokens[i]
            if t in self.reserved_keywords:
                if t == &#34;NOT&#34;:
                    if i == len(tokens) - 1 or tokens[i+1] in self.reserved_keywords:
                        raise Exception(&#34;Not should be followed by a query term&#34;)
                    evaluated_tokens.append(self.look_up.query(tokens[i+1], 1))
                    i += 1
                else:
                    evaluated_tokens.append(t)
            else:
                evaluated_tokens.append(self.look_up.query(t))
            i += 1
        return evaluated_tokens

    def parse(self, s):
        &#34;&#34;&#34;&#34;
        Main method in Parse to parse the query string
        :s
        &#34;&#34;&#34;
        tokens = self.pre_process(s)
        if not self.check_format(tokens):
            raise Exception(&#34;Invalid boolean query&#34;)
        tokens = self.evaluate_query_terms(tokens)
        tokens_stack = []
        for t in tokens:
            tokens_stack.append(t)
            if tokens_stack[-1] == &#34;)&#34;:
                local_stack = []
                flag = 0
                while len(tokens_stack) and not flag:
                    local_stack.append(tokens_stack[-1])
                    if tokens_stack[-1] == &#34;(&#34;:
                        flag = 1
                    tokens_stack.pop()
                if len(local_stack) == 3:
                    tokens_stack.append(local_stack[1])
                else:
                    if local_stack[2] == &#34;AND&#34;:
                        tokens_stack.append(Postings.logical_and(local_stack[1], local_stack[3]))
                    else:
                        tokens_stack.append(Postings.logical_or(local_stack[1], local_stack[3]))
        return tokens_stack[0]


class LookUp:
    &#34;&#34;&#34;&#34;
    LookUp Class
    This class is used to search for a term from inverted index and permuterm index
    It takes care of wildcard characters and also also spelling mistakes by checking the best edit distance terms.
    &#34;&#34;&#34;
    def __init__(self):
        self.inverted_index = InvertedIndex()
        self.permuterm_index = PermutermIndex()
        self.data_set = Dataset()

    @staticmethod
    def _edit_distance(s1, s2):
        &#34;&#34;&#34;
        Method to find the edit distance of two strings
        :cvar
        &#34;&#34;&#34;
        if len(s1) &gt; len(s2):
            s1, s2 = s2, s1
        distances = range(len(s1) + 1)
        for i2, c2 in enumerate(s2):
            distances_ = [i2+1]
            for i1, c1 in enumerate(s1):
                if c1 == c2:
                    distances_.append(distances[i1])
                else:
                    distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))
            distances = distances_
        return distances[-1]

    def _get_negation(self, p_list):
        &#34;&#34;&#34;
        Method to get the negation of a posting list
        :cvar
        &#34;&#34;&#34;
        output = []
        hash_map = {}
        for i in p_list:
            hash_map[i] = 1
        for key in self.data_set.inverted_file_index:
            if key not in hash_map:
                output.append(key)
        return output

    def _get_qualifying_tokens(self, q):
        &#34;&#34;&#34;&#34;
        Checks for spelling mistakes, wildcard characters and returns also qualifying posting lists
        :cvar
        &#34;&#34;&#34;
        output = []
        if q.find(&#39;*&#39;) != -1:
            # wildcard character
            output = self.permuterm_index.get_tokens(q[q.find(&#39;*&#39;)+1:] + &#34;$&#34; + q[0:q.find(&#39;*&#39;)])
        elif not self.permuterm_index.search_token(q + &#34;$&#34;):
            # find closest words using edit distance
            min_ed = 1000000000
            for key in self.inverted_index.index:
                min_ed = min(self._edit_distance(q, key), min_ed)
            for key in self.inverted_index.index:
                ed = self._edit_distance(q, key)
                if ed == min_ed:
                    output.append(key)
        else:
            output = [q]
        return output

    def query(self, q, neg=0):
        &#34;&#34;&#34;
        Returns the posting list for a term.
        Returns negation if neg = 1
        :param q:
        :param neg:
        :return:
        &#34;&#34;&#34;
        output = []
        tokens = self._get_qualifying_tokens(q)
        for t in tokens:
            postings = self.inverted_index.get_postings(t)
            output = Postings.logical_or(output, postings)
        if neg:
            return self._get_negation(output)
        return output


class Search:
    def __init__(self):
        self.parser = Parser()
        self.data_set = Dataset()

    def find(self, q):
        result = self.parser.parse(q)
        for i in range(len(result)):
            result[i] = self.data_set.inverted_file_index[result[i]]
        return result</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="search.tokenize"><code class="name flex">
<span>def <span class="ident">tokenize</span></span>(<span>s)</span>
</code></dt>
<dd>
<div class="desc"><p>Method to tokenize a string
:cvar</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tokenize(s):
    &#34;&#34;&#34;
    Method to tokenize a string
    :cvar
    &#34;&#34;&#34;
    i = 0
    length = len(s)
    tokens = []
    temp = &#34;&#34;
    while i &lt; length:
        if s[i] == &#39; &#39; or s[i] == &#39;(&#39; or s[i] == &#39;)&#39;:
            if len(temp):
                tokens.append(temp)
            if s[i] == &#39;(&#39; or s[i] == &#39;)&#39;:
                tokens.append(s[i])
            temp = &#34;&#34;
        else:
            temp += s[i]
        i += 1
    if len(temp):
        tokens.append(temp)
    return tokens</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="search.LookUp"><code class="flex name class">
<span>class <span class="ident">LookUp</span></span>
</code></dt>
<dd>
<div class="desc"><p>"
LookUp Class
This class is used to search for a term from inverted index and permuterm index
It takes care of wildcard characters and also also spelling mistakes by checking the best edit distance terms.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LookUp:
    &#34;&#34;&#34;&#34;
    LookUp Class
    This class is used to search for a term from inverted index and permuterm index
    It takes care of wildcard characters and also also spelling mistakes by checking the best edit distance terms.
    &#34;&#34;&#34;
    def __init__(self):
        self.inverted_index = InvertedIndex()
        self.permuterm_index = PermutermIndex()
        self.data_set = Dataset()

    @staticmethod
    def _edit_distance(s1, s2):
        &#34;&#34;&#34;
        Method to find the edit distance of two strings
        :cvar
        &#34;&#34;&#34;
        if len(s1) &gt; len(s2):
            s1, s2 = s2, s1
        distances = range(len(s1) + 1)
        for i2, c2 in enumerate(s2):
            distances_ = [i2+1]
            for i1, c1 in enumerate(s1):
                if c1 == c2:
                    distances_.append(distances[i1])
                else:
                    distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))
            distances = distances_
        return distances[-1]

    def _get_negation(self, p_list):
        &#34;&#34;&#34;
        Method to get the negation of a posting list
        :cvar
        &#34;&#34;&#34;
        output = []
        hash_map = {}
        for i in p_list:
            hash_map[i] = 1
        for key in self.data_set.inverted_file_index:
            if key not in hash_map:
                output.append(key)
        return output

    def _get_qualifying_tokens(self, q):
        &#34;&#34;&#34;&#34;
        Checks for spelling mistakes, wildcard characters and returns also qualifying posting lists
        :cvar
        &#34;&#34;&#34;
        output = []
        if q.find(&#39;*&#39;) != -1:
            # wildcard character
            output = self.permuterm_index.get_tokens(q[q.find(&#39;*&#39;)+1:] + &#34;$&#34; + q[0:q.find(&#39;*&#39;)])
        elif not self.permuterm_index.search_token(q + &#34;$&#34;):
            # find closest words using edit distance
            min_ed = 1000000000
            for key in self.inverted_index.index:
                min_ed = min(self._edit_distance(q, key), min_ed)
            for key in self.inverted_index.index:
                ed = self._edit_distance(q, key)
                if ed == min_ed:
                    output.append(key)
        else:
            output = [q]
        return output

    def query(self, q, neg=0):
        &#34;&#34;&#34;
        Returns the posting list for a term.
        Returns negation if neg = 1
        :param q:
        :param neg:
        :return:
        &#34;&#34;&#34;
        output = []
        tokens = self._get_qualifying_tokens(q)
        for t in tokens:
            postings = self.inverted_index.get_postings(t)
            output = Postings.logical_or(output, postings)
        if neg:
            return self._get_negation(output)
        return output</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="search.LookUp.query"><code class="name flex">
<span>def <span class="ident">query</span></span>(<span>self, q, neg=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the posting list for a term.
Returns negation if neg = 1
:param q:
:param neg:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def query(self, q, neg=0):
    &#34;&#34;&#34;
    Returns the posting list for a term.
    Returns negation if neg = 1
    :param q:
    :param neg:
    :return:
    &#34;&#34;&#34;
    output = []
    tokens = self._get_qualifying_tokens(q)
    for t in tokens:
        postings = self.inverted_index.get_postings(t)
        output = Postings.logical_or(output, postings)
    if neg:
        return self._get_negation(output)
    return output</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="search.Parser"><code class="flex name class">
<span>class <span class="ident">Parser</span></span>
</code></dt>
<dd>
<div class="desc"><p>Parser class parses a search string, retrives the required postings lists and returns the final search result.
It also performs stop-word removal, stemming and tokenization of the search terms.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Parser:
    &#34;&#34;&#34;
    Parser class parses a search string, retrives the required postings lists and returns the final search result.
    It also performs stop-word removal, stemming and tokenization of the search terms.
    &#34;&#34;&#34;
    def __init__(self):
        self.ps = PorterStemmer()
        self.look_up = LookUp()
        self.reserved_keywords = [&#34;AND&#34;, &#34;OR&#34;, &#34;NOT&#34;, &#34;(&#34;, &#34;)&#34;]

    def pre_process(self, s):
        &#34;&#34;&#34;
        Removes the stop words, stems and tokenize the tokens
        :param s:
        :return:
        &#34;&#34;&#34;
        if len(s) and s[0] != &#39;(&#39;:
            s = &#34;(&#34; + s + &#34;)&#34;
        tokens = tokenize(s)
        for i in range(len(tokens)):
            if tokens[i] not in self.reserved_keywords:
                tokens[i] = self.ps.stem(tokens[i].lower())
        return tokens

    @staticmethod
    def check_format(tokens):
        &#34;&#34;&#34;&#34;
        Checks if the format of the query is correct
        :tokens
        &#34;&#34;&#34;
        tokens_stack = []
        for t in tokens:
            tokens_stack.append(t)
            if tokens_stack[-1] == &#34;)&#34;:
                flag = 0
                while len(tokens_stack) and not flag:
                    if tokens_stack[-1] == &#34;(&#34;:
                        flag = 1
                    tokens_stack.pop()
                if flag == 0:
                    return False
        return True

    def evaluate_query_terms(self, tokens):
        &#34;&#34;&#34;
        Evaluate the query terms, replaces the keywords by the posting lists
        :cvar
        &#34;&#34;&#34;
        evaluated_tokens = []
        i = 0
        while i &lt; len(tokens):
            t = tokens[i]
            if t in self.reserved_keywords:
                if t == &#34;NOT&#34;:
                    if i == len(tokens) - 1 or tokens[i+1] in self.reserved_keywords:
                        raise Exception(&#34;Not should be followed by a query term&#34;)
                    evaluated_tokens.append(self.look_up.query(tokens[i+1], 1))
                    i += 1
                else:
                    evaluated_tokens.append(t)
            else:
                evaluated_tokens.append(self.look_up.query(t))
            i += 1
        return evaluated_tokens

    def parse(self, s):
        &#34;&#34;&#34;&#34;
        Main method in Parse to parse the query string
        :s
        &#34;&#34;&#34;
        tokens = self.pre_process(s)
        if not self.check_format(tokens):
            raise Exception(&#34;Invalid boolean query&#34;)
        tokens = self.evaluate_query_terms(tokens)
        tokens_stack = []
        for t in tokens:
            tokens_stack.append(t)
            if tokens_stack[-1] == &#34;)&#34;:
                local_stack = []
                flag = 0
                while len(tokens_stack) and not flag:
                    local_stack.append(tokens_stack[-1])
                    if tokens_stack[-1] == &#34;(&#34;:
                        flag = 1
                    tokens_stack.pop()
                if len(local_stack) == 3:
                    tokens_stack.append(local_stack[1])
                else:
                    if local_stack[2] == &#34;AND&#34;:
                        tokens_stack.append(Postings.logical_and(local_stack[1], local_stack[3]))
                    else:
                        tokens_stack.append(Postings.logical_or(local_stack[1], local_stack[3]))
        return tokens_stack[0]</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="search.Parser.check_format"><code class="name flex">
<span>def <span class="ident">check_format</span></span>(<span>tokens)</span>
</code></dt>
<dd>
<div class="desc"><p>"
Checks if the format of the query is correct
:tokens</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def check_format(tokens):
    &#34;&#34;&#34;&#34;
    Checks if the format of the query is correct
    :tokens
    &#34;&#34;&#34;
    tokens_stack = []
    for t in tokens:
        tokens_stack.append(t)
        if tokens_stack[-1] == &#34;)&#34;:
            flag = 0
            while len(tokens_stack) and not flag:
                if tokens_stack[-1] == &#34;(&#34;:
                    flag = 1
                tokens_stack.pop()
            if flag == 0:
                return False
    return True</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="search.Parser.evaluate_query_terms"><code class="name flex">
<span>def <span class="ident">evaluate_query_terms</span></span>(<span>self, tokens)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate the query terms, replaces the keywords by the posting lists
:cvar</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate_query_terms(self, tokens):
    &#34;&#34;&#34;
    Evaluate the query terms, replaces the keywords by the posting lists
    :cvar
    &#34;&#34;&#34;
    evaluated_tokens = []
    i = 0
    while i &lt; len(tokens):
        t = tokens[i]
        if t in self.reserved_keywords:
            if t == &#34;NOT&#34;:
                if i == len(tokens) - 1 or tokens[i+1] in self.reserved_keywords:
                    raise Exception(&#34;Not should be followed by a query term&#34;)
                evaluated_tokens.append(self.look_up.query(tokens[i+1], 1))
                i += 1
            else:
                evaluated_tokens.append(t)
        else:
            evaluated_tokens.append(self.look_up.query(t))
        i += 1
    return evaluated_tokens</code></pre>
</details>
</dd>
<dt id="search.Parser.parse"><code class="name flex">
<span>def <span class="ident">parse</span></span>(<span>self, s)</span>
</code></dt>
<dd>
<div class="desc"><p>"
Main method in Parse to parse the query string
:s</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse(self, s):
    &#34;&#34;&#34;&#34;
    Main method in Parse to parse the query string
    :s
    &#34;&#34;&#34;
    tokens = self.pre_process(s)
    if not self.check_format(tokens):
        raise Exception(&#34;Invalid boolean query&#34;)
    tokens = self.evaluate_query_terms(tokens)
    tokens_stack = []
    for t in tokens:
        tokens_stack.append(t)
        if tokens_stack[-1] == &#34;)&#34;:
            local_stack = []
            flag = 0
            while len(tokens_stack) and not flag:
                local_stack.append(tokens_stack[-1])
                if tokens_stack[-1] == &#34;(&#34;:
                    flag = 1
                tokens_stack.pop()
            if len(local_stack) == 3:
                tokens_stack.append(local_stack[1])
            else:
                if local_stack[2] == &#34;AND&#34;:
                    tokens_stack.append(Postings.logical_and(local_stack[1], local_stack[3]))
                else:
                    tokens_stack.append(Postings.logical_or(local_stack[1], local_stack[3]))
    return tokens_stack[0]</code></pre>
</details>
</dd>
<dt id="search.Parser.pre_process"><code class="name flex">
<span>def <span class="ident">pre_process</span></span>(<span>self, s)</span>
</code></dt>
<dd>
<div class="desc"><p>Removes the stop words, stems and tokenize the tokens
:param s:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pre_process(self, s):
    &#34;&#34;&#34;
    Removes the stop words, stems and tokenize the tokens
    :param s:
    :return:
    &#34;&#34;&#34;
    if len(s) and s[0] != &#39;(&#39;:
        s = &#34;(&#34; + s + &#34;)&#34;
    tokens = tokenize(s)
    for i in range(len(tokens)):
        if tokens[i] not in self.reserved_keywords:
            tokens[i] = self.ps.stem(tokens[i].lower())
    return tokens</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="search.Postings"><code class="flex name class">
<span>class <span class="ident">Postings</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Postings:
    @staticmethod
    def logical_and(l1, l2):
        &#34;&#34;&#34;
        Perform logical and of two lists
        :param l1:
        :param l2:
        :return:
        &#34;&#34;&#34;
        output = []
        i = 0
        j = 0
        len1 = len(l1)
        len2 = len(l2)
        while i &lt; len1 and j &lt; len2:
            if l1[i] == l2[j]:
                output.append(l1[i])
                i += 1
                j += 1
            elif l1[i] &lt; l2[j]:
                i += 1
            else:
                j += 1
        return output

    @staticmethod
    def logical_or(l1, l2):
        &#34;&#34;&#34;
        Performs logical or of two lists
        :param l1:
        :param l2:
        :return:
        &#34;&#34;&#34;
        output = []
        hash_map = {}
        for i in l1:
            hash_map[i] = 1
        for i in l2:
            hash_map[i] = 1
        for key in hash_map:
            output.append(key)
        return output</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="search.Postings.logical_and"><code class="name flex">
<span>def <span class="ident">logical_and</span></span>(<span>l1, l2)</span>
</code></dt>
<dd>
<div class="desc"><p>Perform logical and of two lists
:param l1:
:param l2:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def logical_and(l1, l2):
    &#34;&#34;&#34;
    Perform logical and of two lists
    :param l1:
    :param l2:
    :return:
    &#34;&#34;&#34;
    output = []
    i = 0
    j = 0
    len1 = len(l1)
    len2 = len(l2)
    while i &lt; len1 and j &lt; len2:
        if l1[i] == l2[j]:
            output.append(l1[i])
            i += 1
            j += 1
        elif l1[i] &lt; l2[j]:
            i += 1
        else:
            j += 1
    return output</code></pre>
</details>
</dd>
<dt id="search.Postings.logical_or"><code class="name flex">
<span>def <span class="ident">logical_or</span></span>(<span>l1, l2)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs logical or of two lists
:param l1:
:param l2:
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def logical_or(l1, l2):
    &#34;&#34;&#34;
    Performs logical or of two lists
    :param l1:
    :param l2:
    :return:
    &#34;&#34;&#34;
    output = []
    hash_map = {}
    for i in l1:
        hash_map[i] = 1
    for i in l2:
        hash_map[i] = 1
    for key in hash_map:
        output.append(key)
    return output</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="search.Search"><code class="flex name class">
<span>class <span class="ident">Search</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Search:
    def __init__(self):
        self.parser = Parser()
        self.data_set = Dataset()

    def find(self, q):
        result = self.parser.parse(q)
        for i in range(len(result)):
            result[i] = self.data_set.inverted_file_index[result[i]]
        return result</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="search.Search.find"><code class="name flex">
<span>def <span class="ident">find</span></span>(<span>self, q)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find(self, q):
    result = self.parser.parse(q)
    for i in range(len(result)):
        result[i] = self.data_set.inverted_file_index[result[i]]
    return result</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="search.tokenize" href="#search.tokenize">tokenize</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="search.LookUp" href="#search.LookUp">LookUp</a></code></h4>
<ul class="">
<li><code><a title="search.LookUp.query" href="#search.LookUp.query">query</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="search.Parser" href="#search.Parser">Parser</a></code></h4>
<ul class="">
<li><code><a title="search.Parser.check_format" href="#search.Parser.check_format">check_format</a></code></li>
<li><code><a title="search.Parser.evaluate_query_terms" href="#search.Parser.evaluate_query_terms">evaluate_query_terms</a></code></li>
<li><code><a title="search.Parser.parse" href="#search.Parser.parse">parse</a></code></li>
<li><code><a title="search.Parser.pre_process" href="#search.Parser.pre_process">pre_process</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="search.Postings" href="#search.Postings">Postings</a></code></h4>
<ul class="">
<li><code><a title="search.Postings.logical_and" href="#search.Postings.logical_and">logical_and</a></code></li>
<li><code><a title="search.Postings.logical_or" href="#search.Postings.logical_or">logical_or</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="search.Search" href="#search.Search">Search</a></code></h4>
<ul class="">
<li><code><a title="search.Search.find" href="#search.Search.find">find</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>